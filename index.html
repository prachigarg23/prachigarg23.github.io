---
layout: default
tags: about
---
<!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
<script type="text/javascript">
function readMore() {
    $('#readMore').hide();
    $('#more').show();
}
function readLess() {
    $('#readMore').show();
    $('#more').hide();
}
</script> -->

<style>
      html * {
        font-size: 13px;
      }
      #thetable td:first-child {
        width: 20px;
    }

      /* a:link {
        color: blue;
        background-color: transparent;
        text-decoration: none;
      } */
</style>

<!-- <a href="https://www.cs.cmu.edu/~ftorre/">Prof. Fernando De La Torre</a> -->

<img src="images/me_sf.jpg" alt="Prachi Garg" width="250" style="float: right; padding: 12px; border-radius: 45%;" />

<div class="bio" style="text-align:justify">

<p> Hi! I have started my Ph.D. in Computer Science at the University of Illinois Urbana Champaign, where I am advised by <a href="https://dhoiem.cs.illinois.edu/">Prof. Derek Hoiem</a>. I like to build general purpose and adaptable vision and robotics systems, particularly (i) models that can continually acquire new skills and concepts over time, and (ii) enable user personalization through interaction and feedback.</p>

<p>Previously, I finished my Masters of Science in Robotics from Carnegie Mellon University. For my thesis research, I worked on <i>continual personalization of human action recognition</i> advised by <a href="https://www.cs.cmu.edu/~ftorre/">Prof. Fernando De La Torre</a> and in collaboration with Meta Reality Labs, XR Input Team. I spent summer 2024 building the multi-view camera-LiDAR 3D perception pipeline for the US Department of Transportation,  Safe Intersection Challenge with <a href="https://www.cs.cmu.edu/~srinivas/">Prof. Srinivasa Narsimhan</a>.

<p>In the past, I have had wonderful opportunities working with <a href="https://www.iith.ac.in/~vineethnb/">Prof. Vineeth N Balasubramanian</a> (CMU and IIIT-Hyderabad), <a href="https://faculty.iiit.ac.in/~jawahar/">Prof. C V Jawahar</a> (IIIT-Hyderabad), and <a href="https://scholar.google.com/citations?hl=en&user=Gb5a92sAAAAJ&view_op=list_works&sortby=pubdate">Prof. Frederic Jurie</a> (in beautiful Normandy, France).
  <!-- I completed my Bachelors in Computer Science and Engineering in 2020 from Delhi College of Engineering, India. -->
</p>
<br>
<p>
<!-- Even before, I've had the pleasure of conducting research at:
<ul>
  <li>
    <p><a href="https://research.ibm.com/">IBM Research AI</a>, summer 2020 with <a href="https://researcher.watson.ibm.com/researcher/view.php?person=in-sameepmehta">Dr. Sameep Mehta</a> and <a href="https://researcher.watson.ibm.com/researcher/view.php?person=in-nishthamadaan">Nishtha Madaan</a>.</p>
  </li>
  <li>
    <p>Bachelors thesis project advised by <a href="https://dtu.irins.org/profile/66871#personal_information_panel">Prof. Rajni Jindal</a>. I was curious about geometric deep learning and explored multi-label node classification using GNNs.
  </li>
  <li>
    <p><a href="https://www.greyc.fr/en/home/">Image Team GREYC</a>, <a href="https://www.unicaen.fr/">University of Caen Normandy</a>, <a href="http://www.cnrs.fr/">CNRS</a> (France), summer 2019 with <a href="https://scholar.google.com/citations?hl=en&user=Gb5a92sAAAAJ&view_op=list_works&sortby=pubdate">Prof. Frederic Jurie</a> and <a href="https://lechervy.users.greyc.fr/index.php#">Prof. Alexis Lechervy</a>. This summer spent doing research with Prof. Frederic Jurie in beautiful Normandy (France) made me want to pursue research long term!</p>
  </li>
  <li>
    <p><a href="https://www.iiitd.ac.in/">IIITD</a>, in my 3rd year of Bachelors with <a href="https://www.iiitd.ac.in/anands">Prof. Saket Anand</a>. I had the opportunity to work on Domain Generalization for Animal Detection on the Caltech Camera Traps Dataset (for visual wildlife monitoring applications).
    </p>
  </li>
</ul> -->
</p>

<!-- <p><strong>Research Interests: <span style="color:gray">I'm interested in studying how deep neural networks can be made to generalize well to novel environments and visual scenes. My areas of interest include continual (life-long) and meta learning, domain adaptation, improving generalization in neural networks, multi-task learning and visual scene understanding. I'm excited about autonomous driving and aerial robotics. I find geometric deep learning and graph neural networks to be really fascinating. I'm attracted towards challenging AI applications with a social impact.</p></span></strong> -->

<!-- <p>I love books and cinema, so much that I often find myself tracing movies back to their cast and analysing them. I enjoy cycling and (try) dancing in my free time. Iâ€™m always up for interesting collaborations or just random chats on AI, feel free to drop me a message on Linkedin or via email.</p> -->

<br/>

<!-- <div class="container">
  <div class="row" style="text-align:center;">
        <div class="col">
          <a href="https://www.ri.cmu.edu/"><img src="images/cmu.png" style="max-height:150px;width:80%"></a>
        </div>
        <div class="col">
          <a href="http://dtu.ac.in/"><img src="images/dtu.png" style="max-height:150px;width:80%"></a>
        </div>
        <div class="col" style="text-align:center;">
          <a href="https://www.iiit.ac.in/"><img src="images/IIITH.png" style="max-height:150px;width:80%"></a>
        </div>
        <div class="col" style="text-align:center;">
          <a href="https://research.ibm.com/"><img src="images/ibm.png" style="width:80%;max-height:150px"></a>
        </div>
        <div class="col">
          <a href="https://www.ensicaen.fr/en/research/greyc/"><img src="images/greyc.png" style="width:80%;max-height:150px"></a>
        </div>
        <div class="col">
          <a href="https://www.iiitd.ac.in/"><img src="images/IIITD.png" style="width:80%;max-height:150px"></a>
        </div>
  </div>
  <div class="row" style="text-align:center;">
        <div class="col">
          <div style="padding:10px"><h6>2022-Present</h6></div>
        </div>
        <div class="col">
          <div style="padding:10px"><h6>2016-2020</h6></div>
        </div>
        <div class="col">
          <div style="padding:10px"><h6>2020-2022</h6></div>
        </div>
        <div class="col">
          <div style="padding:10px"><h6>Summer 2020</h6></div>
        </div>
        <div class="col">
          <div style="padding:10px"><h6>Summer 2019</h6></div>
        </div>
        <div class="col">
          <div style="padding:10px"><h6>Winter 2018</h6></div>
        </div>
  </div>
</div> -->

<!-- <hr/> -->
<br/>

<div id="research">
<h2><a name="research">Recent Publications</a></h2>
<br/>

<table width="100%" align="center" valign="middle" cellspacing="0" cellpadding="0" style="border-collapse: collapse;">
    <tbody>
      <tr>
      <td width="35%">
        <div class="one" style="text-align:center;">
            <img src="images/POET_ECCV_teaser.png" style="max-height: 350px;">
        </div>
      </td>
      <td valign="top" width="65%">
        <h5>
          <font size="+1">POET: Prompt Offset Tuning for Continual Human Action Adaptation</font>
        </h5>
        <p class="authors">
        <font size="-6"><b>Prachi Garg</b>, Joseph K J, Vineeth N B, Necati Cihan Camgoz, Chengde Wan, Kenrick Kin, Weiguang Si, Shugao Ma, Fernando De La Torre</font>
        </p>
        <span>
        <font size="+1"><b>ECCV 2024,</b></font>
        </span>
        <span style="color:red"><b>Oral Presentation (2.32%)</b></span>
        <p>
        <a href="https://humansensinglab.github.io/POET-continual-action-recognition/">Project Page / </a>
        <a href="http://www.humansensing.cs.cmu.edu/sites/default/files/final_ECCV_POET_complete.pdf">Paper / </a>
        <a href="https://github.com/humansensinglab/POET-continual-action-recognition">Code / </a> 
        <a href="https://www.youtube.com/watch?v=oOK0cglVKQk">Talk Video </a>
        </p>

      </td>
    </tr>
    <tr>
    <td width="35%">
      <div class="one" style="text-align:center;">
          <img src="images/ICCV2023_BOATMI.png" style="max-height: 150px;">
      </div>
    </td>
    <td valign="top" width="60%">
      <h5>
        <font size="+1">Data-Free Class-Incremental Hand Gesture Recognition</font>
      </h5>
      <p class="authors">
      <font size="-6">Shubhra Aich*, Jesus Ruiz*, Zhenyu Lu, <b>Prachi Garg</b>, K J Joseph, Alvaro Garcia, Vineeth N B, Kenrick Kin, Chengde Wan, Necati Cihan Camgoz, Shugao Ma, Fernando De La Torre</font>

      </p>
      <p>
      <b>ICCV 2023</b>
      
      </p>
      <p>
      <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Aich_Data-Free_Class-Incremental_Hand_Gesture_Recognition_ICCV_2023_paper.pdf">Paper / </a>
      <a href="https://github.com/humansensinglab/dfcil-hgr">Code </a>
      </p>

    </td>
  </tr>
      <tr>
      <td width="35%">
        <div class="one" style="text-align:center;">
            <img src="images/maindiagram.png" style="max-height: 350px;">
        </div>
      </td>
      <td valign="top" width="65%">
        <h5>
          <font size="+1">Multi-Domain Incremental Learning for Semantic Segmentation</font>
        </h5>
        <p class="authors">
        <font size="-6"><b>Prachi Garg</b>, Rohit Saluja, Vineeth N B, Chetan Arora, Anbumani Subramanian, C V Jawahar</font>
        </p>
        <p>
          <b>WACV 2022</b>
        </p>
        <p>
        <a href="https://openaccess.thecvf.com/content/WACV2022/papers/Garg_Multi-Domain_Incremental_Learning_for_Semantic_Segmentation_WACV_2022_paper.pdf">Paper / </a>
        <!-- <i><a href="https://arxiv.org/abs/2110.12205">arxiv / </a></i> -->
        <a href="https://www.youtube.com/watch?v=YQC5KLZUpyc">Video / </a>
        <a href="https://github.com/prachigarg23/MDIL-SS">Code / </a>
        <a href="reports/294-wacv-poster.pdf">Poster / </a>
        <a href="https://openaccess.thecvf.com/content/WACV2022/supplemental/Garg_Multi-Domain_Incremental_Learning_WACV_2022_supplemental.pdf">Supplementary</a>
        </p>

      </td>
    </tr>
  </tbody>
</table>
</div>

<br/>

<div id="talks">
<h2>News</h2>
<table width="100%" align="center" valign="middle" cellspacing="0" cellpadding="0" style="border-collapse: collapse;">
    <tbody>
    <td valign="middle" width="20%">
      <tr></tr>
        <h5>
        <span style="color:red">[Oct 2024]</span> Attending ECCV in Milan, Italy to give an Oral Presentation talk on POET.
        </h5>
    </tr>
      <tr>
        <h5>
          <span style="color:red">[Sept 2024]</span>  I had a fun summer working on multi-view camera-LiDAR 3D perception with Prof. Srinivas Narsimhan.
        </h5>
    </tr>
    <tr>
      <h5>
        <span style="color:red">[Aug 2024]</span>  I have started my CS Ph.D at UIUC, super excited to work with Prof. Derek Hoiem.
      </h5>
  </tr>
  <tr>
    <h5>
       [July 2024]  My favourite work till date, POET on continually personalizing prompts is accepted to ECCV 2024.
    </h5>
    </tr>
    <tr>
        <h5>
          [May 2024] I successfully defended my Masters thesis. <a href="https://www.ri.cmu.edu/publications/continual-personalization-of-human-actions-to-new-categories-over-time/">Thesis</a>
        </h5>
    </tr>
    <tr>
        <h5>
        [Mar 2024] New blog on CMU AI Summer Scholars mentoring experience. Highly recommended.
        </h5>
    </tr>
    <tr>
        <h5>
        [Jan 2024] CVPR 2024 Reviewer.
        </h5>
    </tr>
    <tr>
        <h5>
        [Nov 2023] Gave a talk on 'Prompt Tuning for Practical Continual Learningâ€™ at the Multi-Modal Foundation Models course. <a href="https://docs.google.com/presentation/d/17j4nKldmV1EdU9884q-VGBMyN9kDXExH0txUPpM87RY/edit?usp=sharing">[Slides]</a>
        </h5>
    </tr>
    <tr>
        <h5>
        [Oct 2023] Presented our work 'Data-Free Class-Incremental Hand Gesture Recognition' at ICCV 2023 in Paris.
        </h5>
    </tr>
    <tr>
        <h5>
        [Oct 2023] Our work on 'Continual Few-Shot Learning for Activity Recognition' using lightweight prompt tuning is under review!
        </h5>
    </tr>
    <tr>
        <h5>
          [Jul 2023] Project Leader at CMU CS Pathways, AI Scholars Summer Program. My high school mentees built their first CV-ML project! <a href="https://drive.google.com/file/d/102xu-aLU5xo_cB-5L4DOE-T0AYQfuZyc/view?usp=sharing">[Slides]</a>
        </h5>
    </tr>
    </td>
  </tbody>
</table>
</div>

<br/>

<div id="research projects">
<h2><a name="research">Selected Research Projects</a></h2>
<br/>

<table width="100%" align="center" valign="middle" cellspacing="0" cellpadding="0" style="border-collapse: collapse;">
    <tbody>
    <tr>
    <td width="35%">
      <div class="one" style="text-align:center;">
          <img src="images/diagram-ibm.png" style="max-height: 200px;">
      </div>
    </td>
    <td valign="top" width="65%">
      <h5>
      Towards an AI Infused System for Objectionable Content Detection in OTT [IBM Research Laboratory]
      </h5>
      <p class="authors">
      <font size="-6"> <b>Prachi Garg</b>, Shivang Chopra, Mudit Saxena, Anshu Yadav, Aditya Atri, Nishtha Madaan, Sameep Mehta</font>

      </p>
      <p>
        <a href="https://www.linkedin.com/pulse/towards-ai-infused-system-personalization-video-content-madaan/?articleId=6699399357223218177">Blog-post / </a>
        <a href="https://drive.google.com/drive/folders/1DVUCCE7OLlA7nFbgsf3VanIH_pZRprrq?usp=sharing">Demo </a>
        <!-- <a href="https://kidify-ibm.herokuapp.com/survey.html">Survey</a> -->
      </p>
      <p>With the substantial increase in the consumption of OTT content in recent years, personalized objectionable content detection and filtering has become pertinent for making movie and TV series content suitable for family or children viewing. We propose an objectionable content detection framework which leverages multiple modalities like (i) videos, (ii) subtitle text and (iii) audio to detect (a) violence, (b) explicit NSFW content, and (c) offensive speech in videos. </p>
      <!-- <a href="https://www.linkedin.com/pulse/towards-ai-infused-system-personalization-video-content-madaan/?articleId=6699399357223218177" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Blog-post</a>
      <a href="https://drive.google.com/drive/folders/1DVUCCE7OLlA7nFbgsf3VanIH_pZRprrq?usp=sharing" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Demo</a>
      <a href="https://kidify-ibm.herokuapp.com/survey.html" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Survey</a> -->
    </td>
  </tr>
  <tr>
  <td width="35%">
    <div class="one" style="text-align:center;">
        <img src="images/greyc-proposed-model.png" style="max-height: 200px;">
    </div>
  </td>
  <td valign="top" width="65%">
    <h5>
    Memorization and Generalization in CNNs using Soft Gating Mechanisms [Image Team GREYC, University of Caen Normandy]
    </h5>
    <p class="authors">
    <font size="-6"><b>Prachi Garg</b>, Shivang Agarwal, Alexis Lechervy, Frederic Jurie</font>
    </p>
    <p>
    <a href="reports/Report-GREYC.pdf">Technical Report / </a>
    <a href="https://github.com/prachigarg23/Memorisation-and-Generalisation-in-Deep-CNNs-Using-Soft-Gating-Mechanisms">Code / </a>
    <a href="reports/report-failed-models-GREYC.pdf">Technical Report, Suboptimal ResNet Gating Mechanisms</a>
    </p>
    <p>A deep neural network learns patterns to hypothesize a large subset of samples that lie in-distribution and it memorises any out-of-distribution samples. While fitting to noise, the generalisation error increases and the DNN performs poorly on test set. In this work, we aim to examine if dedicating different layers to the generalizable and memorizable samples in a DNN could simplify the decision boundary learnt by the network and lead to improved generalization in DNNs. While the initial layers that are common to all examples tend to learn general patterns, we dedicate certain deeper additional layers in the network to memorise the out-of-distribution examples.</p>
    <!-- <a href="reports/Report-GREYC.pdf" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Report</a>
    <a href="https://github.com/prachigarg23/Memorisation-and-Generalisation-in-Deep-CNNs-Using-Soft-Gating-Mechanisms" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Code</a>
    <a href="reports/report-failed-models-GREYC.pdf" class="btn btn-outline-primary btn-sm" role="button" aria-pressed="true">Suboptimal ResNet Gating Mechanisms</a> -->
  </td>
  </tr>
  </tbody>
</table>
</div>

<!-- ML-GAT: Multi-label Node Classification using Enhanced Graph Attention Network
Prachi Garg*, Ashi Gupta*, Rajni Jindal

Many real-world graph based problems require the assignment of more than one label to each node instance in the graph. We study here multi-label node classification using enhanced graph neural networks. We propose a novel architecture, Multi-Label Graph attention Network (ML-GAT) that leverages the applicability of the attention based Graph Attention Network (GAT) to efficient inductive semi-supervised multi-label classification by augmenting complex inter-label and node-label dependencies implicit in the graph structure to the learning process. Our model achieves 15.01% increase over the current state-of-the-art ML-GCN framework for Facebook dataset and 6% increase for the Yeast dataset. We analyse the influence of dropout and training size; and infer the relative importance of node-label and label-label dependencies.

*Denotes equal contribution -->

<!-- Visual Wildlife Monitoring: Domain Generalization for Animal Detection in the Wild
Prachi Garg, Gullal Cheema, Saket Anand

I worked towards benchmarking species detection in camera trap images from unconstrained wild environments to generalise to new environments using state-of-the-art Faster-RCNN variants. The Catech Camera Traps dataset (CCT20) is an unconstrained wild environment camera traps dataset designed to study domain generalization for animal species. It contains test data collected from both, locations that are same as train data (cis) as well as locations different from train data (trans). Factors like illumination, motion blur, occlusion, camouflage and perspective can severely affect the performance of species recognition systems. We bridged the generalization gap between cis-locations (test domain same as train) and trans-locations (unseen test domain) performance from 26.8% to 22.9% by using state-of-the-art Faster-RCNN variants. -->



<!-- <p> I have also worked on Domain Generalization in Animal Detection for visual wiildlife monitoring, and multi-label node classification in Graph Attention Networks.</p> -->
<hr>
<p align="right">
<small>Website template from <a href="https://github.com/johno/pixyll">here</a> and inspired from <a href="https://virajprabhu.github.io/">here.</a> </small></p>
